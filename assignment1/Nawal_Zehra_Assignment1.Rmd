---
title: "Web Scraping Assignment 1"
author: "Nawal Zehra Hasan"
date: "11/14/2021"
output: html_document
---

Prompt:
Create a function that will process one page and return with a data frame with one line data frame or list
Create the links that you want to process, you also can write a function that will extract the links first then save them into a vector.lapply your function to your links, you will get a list of data frames
rbindlist your result into one data frame


To complete this task, I chose the apple careers site to scrape data. The base url can be found [*here*](https://jobs.apple.com/en-us/search?location=united-states-USA).


Loading the required libraries
```{r, echo=TRUE, message=FALSE}
library(rvest)
library(data.table)
library(dplyr)

```

1. I created a function to get one job details and form a single line data set. I chose one job and a few variables related to the job to complete the first part of the assignment.
```{r}
# url for one job
t_url <- "https://jobs.apple.com/en-us/details/200125453/us-business-pro?team=APPST"

get_one_job <- function(t_url){
  
  url <- t_url
  
  t <- read_html(t_url)
#title of one job
  title <- 
    t %>% 
    html_nodes("#jdPostingTitle") %>% 
    html_text()
  
#department of job  
  department <- 
    t %>% 
    html_node("#job-team-name") %>% 
    html_text()
  
#date on which job was posted  
  date <- 
    t %>% 
    html_node("#jobPostDate") %>% 
    html_text()

  #combining to form a dataset  
  df1 <- data.frame(url, title, department, date, stringsAsFactors = TRUE)
  return(df1)
}
```

```{r, echo=TRUE, results='hide'}

#checking function
get_one_job("https://jobs.apple.com/en-us/details/200125453/us-business-pro?team=APPST")
```




2. To complete the second part, I created another function for the entire first page of jobs on apple careers. I added the relative links i.e. urls for each job and then formed a dataset from this.
```{r}
# using the url for the entire page with  many jobs
url <- "https://jobs.apple.com/en-us/search?location=united-states-USA"
get_one_page <- function(url){
  
  t <- read_html(url)
  
# title of the jobs  
  title <- 
    t %>% 
    html_nodes(".table--advanced-search__title") %>% 
    html_text()

# department of the jobs  
  department <- 
    t %>% 
    html_nodes(".table--advanced-search__role") %>% 
    html_text()
  
#date on which job was posted  
  date <- 
    t %>% 
    html_nodes(".table--advanced-search__date") %>% 
    html_text()
  
#relative links for each job on the page  
  rel_links <- 
    t %>% 
    html_nodes(".table--advanced-search__title") %>%
    html_attr("href") 
    links <- paste0("https://jobs.apple.com", rel_links)
  
#combining to form a dataset
  df2 <- data.frame(title, department, date, links, stringsAsFactors = TRUE) 
  return(df2)
}
```

```{r, echo=TRUE, results='hide'}
#checking the function
get_one_page("https://jobs.apple.com/en-us/search?location=united-states-USA")
```




3. Finally, I wanted to get more jobs and their details from the next 2 pages and then form one final data set with all the observations from the 3 pages for all the variables. Using lapply I created a list of data frames by the links and the above created function(get_one_page). Using rbind I combined the 3 data frames of the 3 pages scraped.
```{r}
#function to df
df <- get_one_page("https://jobs.apple.com/en-us/search?location=united-states-USA")

#using paste to get the links of the 3 pages with more jobs
links <- paste0("https://jobs.apple.com/en-us/search?location=united-states-USA&page=",1:3)

#using lapply to get a list of dataframs which equals 3 in our case
list_of_dfs <- lapply(links, get_one_page)

#using rbind to combine 3 dataframes by rows/observations and name is as final_df
final_df <- rbindlist(list_of_dfs, fill = T)
```


```{r}
#showing the first part of the final datafram
head(final_df)
```
